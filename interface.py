import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from lifelines import KaplanMeierFitter
from PIL import Image
import plotly.figure_factory as ff
import plotly.express as px
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# ici on charge les donn√©es
data = pd.read_csv("data/data.csv")
data_clean = pd.read_csv("data/data.csv")

# ici on configure la page principale
st.set_page_config(page_title="Projet Final", layout="wide")

# ici on configure les differents tab de la page
page1, page2, page3, page4, page5 = st.tabs([
    "Accueil",
    "Pr√©traitement des donn√©es",
    "Analyse exploratoire",
    "Visualisation des donn√©es",
    "Synth√®se & Conclusion" 
])

# -----------------------------------------------------------------------------
# Page 1 : Accueil
# -----------------------------------------------------------------------------
with page1:
    # Titre de la page
    st.title("üìä Projet Coffe Shop ")
    st.subheader("Une br√®ve introduction √† notre projet et √† notre √©quipe")
    st.write("Lien du dataset selectionn√© : https://www.kaggle.com/datasets/ahmedmohamed2003/cafe-sales-dirty-data-for-cleaning-training")
    st.divider()  # Ligne de s√©paration √©l√©gante

    # ici on decris le projet
    st.subheader("üîç √Ä propos du projet")
    st.write("""
    Notre projet consiste √† analyser un dataset de transactions provenant d'un coffee shop.
    Nous allons nettoyer les donn√©es, les analyser et les
    visualiser pour en tirer des enseignements pertinents.
    """)   


    def resize_image(image_path, size=(150, 150)):
        image = Image.open(image_path)
        image = image.resize(size) 
        return image

    # ici on presente notre equipe
    st.subheader("üë• √âquipe du projet")
    st.write(" ")

    # ici on liste chaque personne de l'√©quipe avec sa photo
    team = [
        {"nom": "Hiba Rahil", "photo": "photo/Hiba.jpeg"},
        {"nom": "Magdat Djaoid", "photo": "photo/Magdat.jpeg"},
        {"nom": "Manal Lazhar", "photo": "photo/Manal.jpeg"},
        {"nom": "Emma Gilbert", "photo": "photo/Emma.jpeg"},
        {"nom": "Ala√Øs Ervera", "photo": "photo/Ala√Øs.jpeg"},
    ]
    cols = st.columns(len(team))  
    # ici on affiche chaque membre de l'√©quipe avec sa photo
    for col, membre in zip(cols, team):
        with col:
            image = resize_image(membre["photo"])
            st.image(image, width=150)
            st.markdown(f"**{membre['nom']}**")

# -----------------------------------------------------------------------------
# Page 2 : Pr√©traitement des donn√©es
# -----------------------------------------------------------------------------
with page2:
    # Titre de la page
    st.subheader("Pr√©traitement des donn√©es")
    st.write("Voici un aper√ßu des donn√©es utilis√©es pour le traitement. "
         "Cette √©tape nous a permis de mieux comprendre la structure du dataset, notamment gr√¢ce √† la fonction `head()`, "
         "qui affiche les premi√®res valeurs")
    st.write(" ")


    st.write("Premiere valeurs du dataset :")
    st.dataframe(data.head()) # ici on Affiche les premi√®res valeurs du dataset
    st.markdown("---")

    st.subheader("1Ô∏è‚É£ V√©rification des valeurs manquantes")
    st.write("Avant de se lancer dans la partie de Analyse des donn√©es, il est important de v√©rifier si notre dataset"
    " contient des valeurs manquantes. Pour cela, nous avons utilis√© la fonction `isnull().sum()` pour compter le nombre "
    "de valeurs manquantes par colonne et nous avons obtenus √ße resultat :")

    # ici on Calcule le nombre total de valeurs manquantes
    total_missing_values = data.isnull().sum().sum()

    missing_per_column = data.isnull().sum()
    st.dataframe(missing_per_column)
    st.warning(f"On remarque qu'il y a √©norm√©ment de valeurs manquantes dans certaines colonnes, avec un total de **{total_missing_values}** valeurs manquantes.")



    st.subheader("2Ô∏è‚É£ Imputation des valeurs manquantes")
    st.markdown("""
    Pour la gestion de valeurs manquantes on comptait toute les supprimer cependant dans ce dataset il y a trop de valeurs manquantes, 
    supprimer chaque ligne avec une valeur manquante supprimerais trop de lignes et engendrerait une perte de donn√©es non negligeable.  
    Nous allons donc plut√¥t imputer les champs en utilisant les m√©thodes suivantes :
    - **Colonnes num√©riques** : Remplacement par la m√©diane (`fillna(median)`)
    - **Colonnes cat√©gorielles** : Remplacement par la modalit√© la plus fr√©quente (`fillna(mode)`)
    """)
    st.write(" ")

    numerical_cols = data_clean.select_dtypes(include=['int64', 'float64']).columns
    categorical_cols = data_clean.select_dtypes(include=['object', 'category']).columns

    # ici on impute les valeurs manquantes
    # les valeurs num√©riques sont remplac√©es par la m√©diane
    for col in numerical_cols:
        median_value = data_clean[col].median()
        data_clean[col].fillna(median_value, inplace=True)

    # les valeurs cat√©gorielles sont remplac√©es par la valeur la plus fr√©quente
    for col in categorical_cols:
        mode_value = data_clean[col].mode()[0]

        data_clean[col] = data_clean[col].replace(["ERROR", "UNKNOWN", np.nan], mode_value)

    # Calcul du nombre total de valeurs manquantes apr√®s traitement
    total_missing_values = data_clean.isnull().sum().sum()

    # V√©rification des valeurs manquantes par colonne
    missing_per_column = data_clean.isnull().sum()

    # Affichage des r√©sultats 
    st.dataframe(missing_per_column)
    st.success(f"A l'aide du tableau, on remarque qu'on a plus aucune valeur manquante, (nombre de valeur manquantes : **{total_missing_values}** )")
    # Sauvegarde des modifications dans le fichier CSV
    data_clean.to_csv("data/data_clean.csv", index=False)
    

    # Encodage avec des chiffres pour 'item' et 'payment method'
    label_encoders = {}  # Dictionnaire pour stocker les encoders

    for col in ['Item', 'Payment Method', 'Location']:
        le = LabelEncoder()
        data_clean[col] = le.fit_transform(data_clean[col])  # l'encodage commence a 0
        label_encoders[col] = le  # Stocker l'encoder si besoin d'inverser plus tard
    # Sauvegarde des modifications dans le fichier CSV
    data_clean.to_csv("data/data_clean.csv", index=False)

    # Affichage sur Streamlit
    st.subheader(" 3Ô∏è‚É£  Encodage des variables cat√©gorielles")
    st.write("Pour faciliter le traitement des donn√©es, nous avons encod√© les variables cat√©gorielles en utilisant des chiffres. "
             "Voici √† quoi ressemble le dataset apr√®s encodage :")
    st.write(data_clean.head())

    # Affichage des mappings utilis√©s
    st.write("Mappings des valeurs encod√©es :")
    mappings_df = pd.DataFrame({
        "Colonne": [col for col in label_encoders.keys()],
        "Valeurs originales": [list(le.classes_) for le in label_encoders.values()],
        "Valeurs encod√©es": [list(le.transform(le.classes_)) for le in label_encoders.values()]
    })
    st.dataframe(mappings_df)

    # üîÑ Correction des types de donn√©es avant
    st.subheader("4Ô∏è‚É£ Typage de chaque colonne")
    st.write("cette partie consiste √† convertir les types de donn√©es de chaque colonne en fonction de leur contenu. "
             "Nous avons utilis√© la fonction `dtypes` pour afficher les types de donn√©es avant et apr√®s le nettoyage.")
    st.write(" ")

    # S√©paration des colonnes num√©riques, cat√©gorielles et date
    numerical_cols = ['Quantity', 'Price Per Unit', 'Total Spent', 'Item', 'Payment Method','Location'] 
    categorical_cols = ['Transaction ID']
    date_cols = ['Transaction Date']

    # Conversion des types
    for col in numerical_cols:
        data_clean[col] = pd.to_numeric(data_clean[col], errors='coerce')  # Convertit en (int/float)

    for col in categorical_cols:
        data_clean[col] = data_clean[col].astype('category')  # Convertit en cat√©gorie

    for col in date_cols:
        data_clean[col] = pd.to_datetime(data_clean[col], errors='coerce')  # Convertit en datetime et met NaT si erreur

    # Suppression des lignes o√π la date est NaT (erreur de conversion ou vide)
    data_clean = data_clean.dropna(subset=date_cols)
    # Sauvegarde des modifications dans le fichier CSV
    data_clean.to_csv("data/data_clean.csv", index=False)

    # Comparaison apr√®s nettoyage
    col1, col2 = st.columns(2)
    with col1:
        st.warning("Types avant conversion :")
        st.dataframe(data.dtypes)
    with col2:
        st.success("Types apr√®s conversion :")
        st.dataframe(data_clean.dtypes)


# -----------------------------------------------------------------------------
# Page 3 : Analyse exploratoire
# -----------------------------------------------------------------------------

with page3 :
    # Titre de la page
    st.subheader("Analyse exploratoire")
    st.write("statistiques du dataset :")


    info_dict = {
        "Column": data.columns,
        "Non-Null Count": data.count().values,
        "Dtype": [data[col].dtype for col in data.columns]
    }
    data_info = pd.DataFrame(info_dict)
    st.dataframe(data_clean.describe())


    st.subheader("üìå Matrice de Corr√©lation")
    st.write("La matrice de corr√©lation ci-dessous nous permet d‚Äôanalyser la relation entre diff√©rentes "
    "variables de notre dataset. Elle nous indique dans quelle mesure deux variables √©voluent ensemble.")

    with st.expander("üîé Options d'analyse", expanded=False):
            selected_features = st.multiselect(
                "S√©lectionnez les variables pour la matrice de corr√©lation :",
                data_clean.select_dtypes(include=['int64', 'float64']).columns.tolist(),
                default=["Item","Quantity", "Price Per Unit", "Total Spent",
                        "Payment Method", "Location"]
            )

   # V√©rification que les colonnes s√©lectionn√©es existent bien et sont num√©riques
    selected_features = [col for col in selected_features if col in data_clean.columns and data_clean[col].dtype in ['int64', 'float64']]

    # Calcul de la matrice de corr√©lation
    correlation_matrix = data_clean[selected_features].corr()

    # Cr√©ation de la heatmap avec des annotations et une mise en forme corrig√©e
    fig_corr = ff.create_annotated_heatmap(
        z=correlation_matrix.values[::-1], 
        x=list(correlation_matrix.columns),
        y=list(correlation_matrix.index)[::-1],
        colorscale="RdBu",
        annotation_text=np.round(correlation_matrix.values[::-1], 2),
        showscale=True,
        reversescale=True
    )

    fig_corr.update_layout(
        height=700
    )
    st.plotly_chart(fig_corr, use_container_width=True)

    st.subheader("üìä Interpr√©tation de la Matrice de Corr√©lation")
    st.write("""
    L'analyse de la matrice de corr√©lation nous permet de mieux comprendre les relations entre les variables de notre dataset. Voici les principales conclusions que nous pouvons en tirer :
    """)
    
    st.markdown("- ‚úÖ **Plus la quantit√© achet√©e est grande, plus la d√©pense totale est √©lev√©e** "
    "‚Üí Corr√©lation de **0.63** entre `Quantity` et `Total Spent`. Cela signifie qu'en moyenne, acheter "
    "en plus grande quantit√© entra√Æne une augmentation du total d√©pens√©.")
    
    st.markdown("- ‚úÖ **Un prix unitaire plus √©lev√© entra√Æne une d√©pense totale plus importante**  "
    "‚Üí Corr√©lation de **0.61** entre `Price Per Unit` et `Total Spent`. Plus un produit est cher √† l'unit√©, plus le total d√©pens√© a tendance √† √™tre √©lev√©.")
    
    st.markdown("- üîç **Certains items ont tendance √† √™tre plus chers que d‚Äôautres**  "
    "‚Üí Corr√©lation de **0.22** entre `Item` et `Price Per Unit`. Bien que cette relation soit plus faible, elle indique que certains articles sp√©cifiques ont des prix unitaires plus √©lev√©s.")
    
    st.write("""
    En revanche, nous observons que certaines variables, comme `Payment Method` et `Location`, ont des corr√©lations tr√®s faibles avec les autres param√®tres. Cela signifie qu'elles n'ont pas d'impact significatif sur les achats en termes de montants ou de quantit√©s.
    """)

# -----------------------------------------------------------------------------
# Page 4 : Visualisation des donn√©es
# -----------------------------------------------------------------------------
with page4 :
    st.subheader("Visualisation des donn√©es")

    # Dictionnaire pour remapper les valeurs encod√©es vers leurs noms r√©els
    item_mapping = {
        0: "Cake",
        1: "Coffee",
        2: "Cookie",
        3: "Juice",
        4: "Salad",
        5: "Sandwich",
        6: "Smoothie",
        7: "Tea"
    }

    # ici on remplace les valeurs encod√©es par leur nom r√©el
    data_clean['Item_Names'] = data_clean['Item'].map(item_mapping)

    # ici on compte le nombre d‚Äôachats par item
    items_counts = data_clean['Item_Names'].value_counts().reset_index()
    items_counts.columns = ['Item', 'Nombre d‚Äôachats']

    # Cr√©ation du graphique avec Plotly
    fig_items = px.bar(
        items_counts,
        y="Item",
        x="Nombre d‚Äôachats",
        color="Nombre d‚Äôachats",
        title="üìå Histogramme des Items les Plus Achet√©s",
        labels={"Item": "Nom de l‚ÄôItem", "Nombre d‚Äôachats": "Nombre de fois achet√©"},
        text_auto=True
    )

    # Personnalisation du layout
    fig_items.update_layout(
        xaxis=dict(tickangle=45),
        yaxis_title="Nombre d‚Äôachats",
        xaxis_title="Items",
        margin=dict(l=50, r=50, t=50, b=100),
        height=600
    )

    # Affichage du graphique 
    st.plotly_chart(fig_items, use_container_width=True)


    # pareil que le graph avant mais pour les paiement (m√™mes etapes)
    payment_mapping = {
        0: "Cash",
        1: "Credit Card",
        2: "Digital Wallet"
    }
    
    data_clean['Payment Method'] = data_clean['Payment Method'].map(payment_mapping)
    
    payment_counts = data_clean['Payment Method'].value_counts().reset_index()
    payment_counts.columns = ['M√©thode de Paiement', 'Nombre de Transactions']

    fig_payment = px.bar(
        payment_counts,
        x="M√©thode de Paiement",
        y="Nombre de Transactions",
        color="Nombre de Transactions",
        title="üìå R√©partition des M√©thodes de Paiement",
        labels={"M√©thode de Paiement": "Type de Paiement", "Nombre de Transactions": "Nombre de Transactions"},
        text_auto=True
    )

    fig_payment.update_layout(
        xaxis_title="M√©thode de Paiement",
        yaxis_title="Nombre de Transactions",
        margin=dict(l=50, r=50, t=50, b=100),
        height=600
    )

    st.plotly_chart(fig_payment, use_container_width=True)

    # graphique de density plot avec plotly
    fig = px.density_heatmap(data_clean, x="Total Spent", y="Quantity", marginal_x="histogram", marginal_y="histogram",
                             title="üìå Distribution des D√©penses Totales et Quantit√©s")
    fig.update_layout(
        height=600
    )
    st.plotly_chart(fig, use_container_width=True)

# -----------------------------------------------------------------------------
# Page 5 : Synth√®se & Conclusion
# -----------------------------------------------------------------------------
    with page5:
        st.subheader("Synthese & Conclusion")
        st.write("""
        Travailler en √©quipe sur ce projet a √©t√© une exp√©rience enrichissante, nous permettant d‚Äôexplorer les diff√©rentes
        √©tapes du traitement des donn√©es. Le pr√©traitement des donn√©es nous a particuli√®rement marqu√© par sa complexit√© 
        et le temps qu‚Äôil a n√©cessit√©. 
        """)
        st.subheader("Enseignements et Perspectives")
        st.write("""
        Gr√¢ce √† notre analyse, nous avons pu d√©gager plusieurs tendances int√©ressantes sur
        les ventes du coffee shop. Nous avons constat√© que plus la quantit√© achet√©e est √©lev√©e, plus le prix total est 
        influenc√©, ce qui est logique. De m√™me, un prix unitaire plus √©lev√© impacte directement le co√ªt final. Du c√¥t√© 
        des produits, le jus s‚Äôest r√©v√©l√© √™tre l‚Äôarticle le plus achet√©, et le mode de paiement le plus utilis√© est le 
        Digital Wallet, avec une transaction moyenne avoisinant 3,9 $.
        """)
        st.write(" ")
        st.write("""Enfin, afin de rendre notre travail plus interactif et accessible, nous avons choisi de d√©velopper une interface 
        utilisateur plut√¥t que de simplement rendre un Jupyter Notebook. Nous avons trouv√© cette approche plus intuitive 
        et int√©ressante pour la visualisation et l‚Äôinterpr√©tation des r√©sultats.""")

        st.success("En fin de compte, nous sommes fiers d‚Äôavoir pu √©tudier ce dataset sur les coffee shops. "
        "Cela nous permettra de continuer √† boire du caf√© et de discuter entre nous au lieu de travailler.ü§°")


